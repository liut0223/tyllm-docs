

# æ›´æ–°è¯´æ˜

æœ¬æ–‡è®°å½•äº†``Edge10``ç³»åˆ—å¤§æ¨¡å‹å·¥å…·é“¾çš„å˜æ›´æƒ…å†µã€‚

**20251128/v1.1.7**

- ğŸš€v1.1.7æ­£å¼ç‰ˆæœ¬å‘å¸ƒ
- ğŸš€ç‹¬ç«‹çš„é‡åŒ–å·¥å…·é•œåƒ
- ğŸš€ç‹¬ç«‹çš„ç¼–è¯‘å·¥å…·é•œåƒ
- ğŸš€æ›´æ–°å¤§æ¨¡å‹æ”¯æŒåˆ—è¡¨
- ğŸš€æ›´æ–°é‡åŒ–å·¥å…·ä½¿ç”¨è¯´æ˜
- ğŸš€æ›´æ–°ç¼–è¯‘å·¥å…·ä½¿ç”¨è¯´æ˜

**20250414/v1.0.8**

- ğŸš€ä¼˜åŒ–``Qwen2.5-VL-7B`` ``ViT``éƒ¨åˆ†æ€§èƒ½

**20250320/v1.0.6**

- ğŸš€æ–°å¢æ”¯æŒ``Qwen2.5-VL-7B``æ¨¡å‹``3Die``ç¼–è¯‘

**20250305/v0.0.2**

- ğŸš€æ–°å¢æ”¯æŒ``Qwen2.5-VL-7B``æ¨¡å‹``4Die/1Die``ç¼–è¯‘


<br>

# æ•´ä½“ä»‹ç»

``TyQuant``ä¸ºäº‘å¤©åŠ±é£å¤§æ¨¡å‹é‡åŒ–å·¥å…·ï¼Œç”¨æˆ·å¯é€šè¿‡æœ¬å·¥å…·æˆ–å…¶å®ƒå¼€æºé‡åŒ–å·¥å…·å¯¹å¤§æ¨¡å‹å®Œæˆé‡åŒ–ï¼›``TyLLM``æ˜¯äº‘å¤©åŠ±é£æ¨å‡ºçš„å¤§æ¨¡å‹å·¥å…·é“¾ï¼Œå¯å¸®åŠ©ç”¨æˆ·å°†å¤§æ¨¡å‹ç¼–è¯‘ä¸º``Edge10``ç³»åˆ—èŠ¯ç‰‡ä¸Šæ‰§è¡Œçš„æ¨¡å‹ã€‚``TyLLM``ä¸ºäº‘å¤©åŸºäº``TyTVM``å·¥å…·é“¾é’ˆå¯¹å¤§æ¨¡å‹å¢é‡å¼€å‘çš„å·¥å…·ï¼Œä¸»è¦åŸºäº``PyTorch``å’Œ``vLLM``å¯¹å¤§æ¨¡å‹åšä¸“å±å’Œå®šåˆ¶ä¼˜åŒ–ï¼›``TyTVM``ä¸ºäº‘å¤©æ¨¡å‹è½¬æ¢ã€é‡åŒ–ã€ä»¿çœŸã€ç¼–è¯‘å·¥å…·é“¾ï¼Œä¸»è¦è´Ÿè´£å°†æ¨¡å‹ç¼–è¯‘ä¸ºèŠ¯ç‰‡æ‰§è¡Œçš„æ¨¡å‹ã€‚

### æ•´ä½“æ¶æ„å¦‚å›¾ï¼š

<div style="text-align:center;">
  <img src="./assets/whiteboard_exported_image.png" alt="æ¶æ„å›¾" style="width:100%; height:auto;" />
</div>

### æ”¯æŒæ¨¡å‹åˆ—è¡¨

å·²ç»æ”¯æŒçš„æ¨¡å‹å¦‚ä¸‹ï¼ˆåŒ…æ‹¬ä¸é™äºï¼‰ï¼š

| Model                                        | Quant Support | Compile Support |
| :------------------------------------------- | :-----------: | :-------------: |
| Qwen/Qwen3-VL-8B                             |      âœ…       |       âœ…        |
| Qwen/Qwen3-VL-4B                             |      âœ…       |       âœ…        |
| Qwen/Qwen3-4B                                |      âœ…       |       âœ…        |
| Qwen/Qwen3-1.7B                              |      âœ…       |       âœ…        |
| Qwen/Qwen2.5-VL-7B                           |      âœ…       |       âœ…        |
| Qwen/Qwen2.5-VL-3B                           |      âœ…       |       âœ…        |
| Qwen/Qwen2-VL-7B                             |      âŒ       |       âœ…        |
| Qwen/Qwen2-7B                                |      âŒ       |       âœ…        |
| Qwen/Qwen1.5-1.8B                            |      âŒ       |       âœ…        |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B    |      âŒ       |       âœ…        |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-7B      |      âŒ       |       âœ…        |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-32B     |      âŒ       |       âœ…        |
| Llama3-8B                                    |      âŒ       |       âœ…        |
<br>

# å¿«é€Ÿå¼€å§‹

æœ¬èŠ‚ä»‹ç»ä½¿ç”¨``TyLLM``å·¥å…·é“¾å‰çš„å¼€å‘ç¯å¢ƒå‡†å¤‡å·¥ä½œã€‚``TyLLM``ä½¿ç”¨``Docker``å®¹å™¨è¿›è¡Œå·¥å…·é“¾é›†æˆï¼Œç”¨æˆ·å¯é€šè¿‡``Docker``åŠ è½½``TyLLM``é•œåƒæ–‡ä»¶ï¼Œç„¶åè¿›è¡Œæ¨¡å‹é‡åŒ–ã€ç¼–è¯‘ã€è¯„ä¼°(æœªæ¥)ç­‰å·¥ä½œï¼Œå› æ­¤å¼€å‘ç¯å¢ƒå‡†å¤‡é˜¶æ®µéœ€è¦æ­£ç¡®å®‰è£…``Docker``ç¯å¢ƒï¼ŒåŒæ—¶ç›®å‰éœ€è¦é‡åŒ–é˜¶æ®µéœ€è¦``GPU``æ¥åŠ é€Ÿï¼Œä»¥åŠå¤šæ¨¡æ€æ¨¡å‹çš„ç¼–è¯‘ä¾èµ–``vLLM``æ¡†æ¶æ¥æ¨ç†ï¼Œå› æ­¤æš‚æ—¶éœ€è¦``GPU``ã€‚

## ç¯å¢ƒå‡†å¤‡

- **Nvidia GPU**
- **Nvidia Container Toolkit**
- **Docker>19.03**

### å®‰è£…Nvidia GPU é©±åŠ¨

```shell
sudo apt install nvidia-driver-530 # é©±åŠ¨ç‰ˆæœ¬å°½é‡é€‰æ‹©æœ€é«˜
# å®‰è£…å®Œæˆåï¼Œæ‰§è¡Œnvidia-smiå‘½ä»¤æ˜¾ç¤ºå¦‚ä¸‹ï¼Œè¡¨ç¤ºå®‰è£…æˆåŠŸã€‚
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.179                Driver Version: 535.179      CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:84:00.0 Off |                  N/A |
| 30%   27C    P8              23W / 350W |      3MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

### å®‰è£…Docker

```shell
sudo apt install docker.io
sudo docker -v
# Docker version 20.10.21, build 20.10.21-0ubuntu1~20.04.2
```

### å®‰è£…Nvidia Container Toolkit

æ·»åŠ åŒ…ä»“åº“å’Œ``GPG key``:
```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
    && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
    && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
             sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
             sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

æ›´æ–°æºï¼Œå®‰è£…nvidia-container-toolkit

```shell
sudo apt update
sudo apt install nvidia-container-toolkit
```

### å®‰è£…TyQuanté‡åŒ–å·¥å…·

é‡åŒ–å·¥å…·é•œåƒè·å–é€”å¾„å¦‚ä¸‹ï¼Œè¯·åŠ¡å¿…å°†``${version}``æ›¿æ¢ä¸ºå®é™…å¯¹åº”çš„ç‰ˆæœ¬å·ï¼Œæ¯”å¦‚``v1.0.2``ï¼š

```shell
sudo docker login 113.100.143.90:8091 -u custom -p DE@sz_intellif_2021
sudo docker pull 113.100.143.90:8091/edgex/tyquantize:${version}
```

### å¯åŠ¨é‡åŒ–å·¥å…·é•œåƒ

ä»¥ä¸‹å‘½ä»¤åˆ›å»ºå®¹å™¨ï¼Œå…¶ä¸­``${your_data_dir}``è¡¨ç¤ºå®¿ä¸»æœºä¸­ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œ``${version}``éœ€æ”¹ä¸ºå®é™…ç‰ˆæœ¬``tag``ã€‚
```shell
sudo docker run --gpus all -v ${your_data_dir}:/data -it 113.100.143.90:8091/edgex/tyquantize:${version} bash
```

### å®‰è£…TyLLMå·¥å…·é“¾

ç¼–è¯‘å·¥å…·é“¾è·å–é€”å¾„å¦‚ä¸‹ï¼Œè¯·åŠ¡å¿…å°†``${version}``æ›¿æ¢ä¸ºå®é™…å¯¹åº”çš„å·¥å…·é“¾ç‰ˆæœ¬å·ï¼Œæ¯”å¦‚``v1.1.7``ï¼š

```shell
sudo docker login 113.100.143.90:8091 -u custom -p DE@sz_intellif_2021
sudo docker pull 113.100.143.90:8091/edgex/tyllm:${version}
```

### å¯åŠ¨å·¥å…·é“¾é•œåƒ

ä»¥ä¸‹å‘½ä»¤åˆ›å»ºå®¹å™¨ï¼Œå…¶ä¸­``${your_data_dir}``è¡¨ç¤ºå®¿ä¸»æœºä¸­ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œ``${version}``éœ€æ”¹ä¸ºå®é™…ç‰ˆæœ¬``tag``ã€‚
```shell
sudo docker run --gpus all -v ${your_data_dir}:/data -it 113.100.143.90:8091/edgex/tyllm:${version} bash
```

> **æ³¨æ„**
> 
> éœ€è¦å°†``113.100.143.90:8091``åŠ å…¥``/etc/docker/daemon.json``ä¸­çš„``insecure-registries``å­—æ®µä¸­ï¼Œå¦‚ä¸‹ï¼š
> 
> ```json
> {     
>      "insecure-registries": ["113.100.143.90:8091"]
> }
>  ```
> ä¿®æ”¹åï¼Œé‡å¯``docker``ç”Ÿæ•ˆï¼Œ``sudo systemctl restart docker``



## æ¨¡å‹é‡åŒ–

å»ºè®®ä½¿ç”¨å¸¦GPUçš„ä¸»æœºåŠ å¿«é‡åŒ–é€Ÿåº¦ã€‚

### é‡åŒ–ç¤ºä¾‹1-LLMæ¨¡å‹

ä»¥Qwen3-1.7Bä¸ºä¾‹ï¼Œç›®å‰æš‚æ—¶ä»…æ”¯æŒAWQæ–¹å¼é‡åŒ–
   ```python
   import torch
   from transformers import AutoModelForCausalLM, AutoTokenizer
   from quant_toolchain import (
       get_awq_config,
       get_dataloader,
       quantize_model,
       save_quantized_model,
   )
   from datasets import load_dataset

   # å‡†å¤‡æ¨¡å‹
   org_model_path = "Qwen3-1.7B"
   dst_model_path = "Qwen3-1.7B-AMTC-AWQ"
   model = AutoModelForCausalLM.from_pretrained(org_model_path, torch_dtype=torch.float16)
   tokenizer = AutoTokenizer.from_pretrained(org_model_path)

   # å‡†å¤‡æ ¡å‡†æ•°æ®ï¼ˆå¯æ›¿æ¢ä¸ºè‡ªæœ‰æ•°æ®ï¼‰
   dataset = load_dataset("cnn_dailymail", name="3.0.0", split="train")
   dataloader = get_dataloader(
       dataset,
       tokenizer,
       num_max_orig_samples=128,
       max_sequence_length=512,
       column="article",
       concat_data=True,
       pad_to_max_length=False,
   )

   # é…ç½®é‡åŒ–è¶…å‚å¹¶ç”Ÿæˆé‡åŒ–æ¨¡å‹
   quant_config = get_awq_config()
   quantized_model = quantize_model(model, quant_config, dataloader, tokenizer)

   # ä¿å­˜é‡åŒ–æ¨¡å‹
   save_quantized_model(quantized_model, dst_model_path)
   ```

### é‡åŒ–ç¤ºä¾‹2-VLMæ¨¡å‹

ä»¥Qwen2.5-vl-7Bä¸ºä¾‹ï¼Œç›®å‰æš‚æ—¶ä»…æ”¯æŒVLMå¤šæ¨¡æ€æ¨¡å‹çš„language mdoelçš„AWQæ–¹å¼é‡åŒ–ï¼Œvisual modelçš„AWQé‡åŒ–æš‚æ—¶ä¸æ”¯æŒã€‚

   ```python
    import os
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModelForImageTextToText
    from quant_toolchain import quantize_model, save_quantized_model, load_quantized_model
    from quant_toolchain.configs.dataset_utils import get_dataloader
    from quant_toolchain.configs.get_config import get_awq_config
    from accelerate import infer_auto_device_map
    from accelerate.big_modeling import dispatch_model
    from datasets import load_dataset, load_from_disk

    MODEL_PATH = "Qwen/Qwen2.5-VL-7B-Instruct"

    SAVE_PATH = "quantized_models/Qwen2.5-VL-7B-Instruct-AMTC-LM-AWQ"
    
    if not os.path.exists(SAVE_PATH): 
        os.makedirs(SAVE_PATH) 
        print(f"ç›®å½• {SAVE_PATH} å·²åˆ›å»º")
    else:
        print(f"ç›®å½• {SAVE_PATH} å·²å­˜åœ¨")
    model = AutoModelForImageTextToText.from_pretrained(MODEL_PATH, torch_dtype=torch.float16) 
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                },
                {"type": "text", "text": "Describe this image."},
            ],
        }
    ]
    # å¦‚ä¸ä½¿ç”¨gpuæœºå™¨é‡åŒ–ï¼Œè¿™é‡Œto("cuda")æ”¹æˆto("cpu")
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_dict=True,
        return_tensors="pt"
    ).to("cuda")

    device_map = infer_auto_device_map(model, offload_buffers=True)
    print("device_map:", device_map)
    dispatch_model(model, device_map=device_map, offload_buffers=True)

    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)
    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    print("generated_text:")
    print(generated_text[0])
    model = model.cpu()

    llm_model = model.language_model
    # ä»¥cnn_dailymailæ•°æ®é›†ä¸ºä¾‹ï¼Œå¯ä»¥æ›¿æ¢ä¸ºå…¶å®ƒä»£è¡¨æ€§æ•°æ®
    dataset = load_dataset("cnn_dailymail", name="3.0.0", split="train")
    dataloader = get_dataloader(dataset, 
                                tokenizer,
                                max_sequence_length = 512,
                                column = "article", 
                                concat_data = True, 
                                pad_to_max_length = False,
                                num_max_orig_samples=128)

    quant_config = get_awq_config(apply_clip=True)    
    # å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œé‡åŒ–
    quantized_model = quantize_model(llm_model, quant_config, dataloader)
    device_map = infer_auto_device_map(model, offload_buffers=True)
    print("device_map:", device_map)
    dispatch_model(model, device_map=device_map, offload_buffers=True)

    with torch.no_grad():
        generated_ids_0 = model.generate(**inputs, max_new_tokens=64, do_sample=False)
    generated_text_0 = tokenizer.batch_decode(generated_ids_0, skip_special_tokens=True)
    print("generated_text_0:")
    print(generated_text_0[0])
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    save_quantized_model(model, SAVE_PATH, tokenizer)

   ```
   

   
### æ ¼å¼è½¬æ¢
é‡åŒ–å®Œæˆåéœ€æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚  
   ```shell
   python3 checkpoint_convert.py --src quantized_model_path --dst coverted_model_path --quant_type awq
   ```
å®Œæˆè½¬æ¢åå³å¯è¿›è¡Œæ¨¡å‹ç¼–è¯‘æˆ–ç²¾åº¦è¯„ä¼°ã€‚

### ç²¾åº¦è¯„ä¼°
- EvalScopeå®˜æ–¹é“¾æ¥ï¼š
    https://evalscope.readthedocs.io/zh-cn/latest/index.html
- å®‰è£…ä¸ä¾èµ–ï¼š  
   - vllm 0.11.0 å®‰è£…
      ```shell
        conda create -n vllm python=3.10
        conda activate vllm
        pip install vllm
      ```
   - EvalScopeå®‰è£…
      ```shell
        conda create -n evalscope python=3.10
        conda activate evalscope
        pip install 'evalscope[all]'
      ```
#### LLMæ¨¡å‹è¯„ä¼°

- å¯åŠ¨vllmæ¨¡å‹(Qwen3-1.7bä¸ºä¾‹)ï¼š
    - æµ®ç‚¹æ¨¡å‹å¯åŠ¨
      ```shell
        export LLM_USE_MODELSCOPE=True 
        export CUDA_VISIBLE_DEVICES=0,1
        python -m vllm.entrypoints.openai.api_server \
        --model Qwen3-1.7b \
        --served-model-name qwen3 \
        --trust-remote-code \
        --dtype float16 \
        --max-model-len 8196 \
        --gpu-memory-utilization 0.5 \
        --max-num-seqs 16 \
        --port 8000
      ```
    - é‡åŒ–æ¨¡å‹å¯åŠ¨
      ```shell
        export LLM_USE_MODELSCOPE=True 
        export CUDA_VISIBLE_DEVICES=2,3
        python -m vllm.entrypoints.openai.api_server \
        --model Qwen3-1.7b-AMTC-AWQ \
        --quantization awq \
        --served-model-name qwen3_awq \
        --trust-remote-code \
        --dtype float16 \
        --max-model-len 8196 \
        --gpu-memory-utilization 0.5 \
        --max-num-seqs 16 \
        --port 8001
      ```
- å¯åŠ¨æ¨¡å‹è¯„ä¼°è„šæœ¬(Qwen3-1.7bä¸ºä¾‹):
    - æ¨¡å‹è¯„ä¼°è„šæœ¬å¯åŠ¨ï¼š
      ```python
      from evalscope import TaskConfig, run_task

      # ä»¥æµ®ç‚¹æ¨¡å‹ä¸ºä¾‹ï¼Œé‡åŒ–æ¨¡å‹éœ€ä¿®æ”¹modelå’Œapi_urlç«¯å£
      task_cfg = TaskConfig(
        model='qwen3',  # é‡åŒ–æ¨¡å‹ä½¿ç”¨model='qwen3_awq'
        api_url='http://127.0.0.1:8000/v1/chat/completions',  #é‡åŒ–æ¨¡å‹ä½¿ç”¨api_url='http://127.0.0.1:8001/v1/chat/completions'
        eval_type='openai_api',
        datasets=['mmlu'],    
        eval_batch_size=32,
        generation_config={
            'max_tokens': 2048,  # æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œå»ºè®®è®¾ç½®ä¸ºè¾ƒå¤§å€¼é¿å…è¾“å‡ºæˆªæ–­
            'temperature': 0.7,  # é‡‡æ ·æ¸©åº¦ (qwen æŠ¥å‘Šæ¨èå€¼)
            'top_p': 0.8,  # top-pé‡‡æ · (qwen æŠ¥å‘Šæ¨èå€¼)
            'top_k': 20,  # top-ké‡‡æ · (qwen æŠ¥å‘Šæ¨èå€¼)
            'n': 1,  # æ¯ä¸ªè¯·æ±‚äº§ç”Ÿçš„å›å¤æ•°é‡
            'extra_body':{'chat_template_kwargs': {'enable_thinking': False}}  # å…³é—­æ€è€ƒæ¨¡å¼
        },
        timeout=60000,  # è¶…æ—¶æ—¶é—´
        stream=True,  # æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        # limit=1000,  # è®¾ç½®ä¸º1000æ¡æ•°æ®è¿›è¡Œæµ‹è¯•
      )
      run_task(task_cfg=task_cfg)
      ```
- LLM ç²¾åº¦åŸºå‡†æš‚ä»¥å…¨é‡ MMLU æ•°æ®é›†ä¸ºæµ‹è¯•é›†:
    - æµ®ç‚¹æ¨¡å‹ç»“æœï¼š
        ```shell
       â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
       | Model   | Dataset   | Metric   | Subset   |   Num |   Score | Cat.0       
       | qwen3   | mmlu      | mean_acc | OVERALL  | 14042 |  0.6224 | -         
       ______________________________________________________________________
        ```
    - é‡åŒ–æ¨¡å‹ç»“æœï¼š
        ```shell
       â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
       | Model   | Dataset   | Metric   | Subset   |   Num |   Score | Cat.0       
       | qwen3   | mmlu      | mean_acc | OVERALL  | 14042 |  0.5901 | -                 
       ______________________________________________________________________
        ```


#### VLMæ¨¡å‹è¯„ä¼°

- å¯åŠ¨vllmæ¨¡å‹(Qwen3-4b-VLä¸ºä¾‹)ï¼š
    - æµ®ç‚¹æ¨¡å‹å¯åŠ¨
      ```shell
        export LLM_USE_MODELSCOPE=True 
        export CUDA_VISIBLE_DEVICES=0,1
        python -m vllm.entrypoints.openai.api_server \
        --model Qwen3-4b-VL \
        --tensor-parallel-size 2 \
        --served-model-name qwen3_vl \
        --trust-remote-code \
        --dtype float16 \
        --max-model-len 10240 \
        --gpu-memory-utilization 0.5 \
        --max-num-seqs 16 \
        --port 8000
      ```
    - é‡åŒ–æ¨¡å‹å¯åŠ¨
      ```shell
        export LLM_USE_MODELSCOPE=True 
        export CUDA_VISIBLE_DEVICES=2,3
        python -m vllm.entrypoints.openai.api_server \
        --model Qwen3-4b-VL-AWQ \
        --tensor-parallel-size 2 \
        --quantization awq \
        --served-model-name qwen3_vl_awq \
        --trust-remote-code \
        --dtype float16 \
        --max-model-len 10240 \
        --gpu-memory-utilization 0.5 \
        --max-num-seqs 16 \
        --port 8001
      ```
- å¯åŠ¨æ¨¡å‹è¯„ä¼°è„šæœ¬(Qwen3-1.7bä¸ºä¾‹):
    - è¯„ä¼°æ¨¡å‹configï¼š  
      ```yaml
       eval_backend: VLMEvalKit
        eval_config:
        model: 
            - type: Qwen3-1.7b-VL ##æŒ‰ç…§å¯åŠ¨è„šæœ¬çš„--model æ¥å¡«
            name: CustomAPIModel 
            api_base: http://localhost:9999/v1/chat/completions
            key: EMPTY
            temperature: 0.0
            img_size: -1
        data:
            - MMStar
        mode: all
        work_dir: outputs
        nproc: 16
        use_cache: /tmp/test
      ```
    - æ¨¡å‹è¯„ä¼°è„šæœ¬å¯åŠ¨ï¼š
      ```python
        from evalscope.run import run_task
        from evalscope.summarizer import Summarizer
        import os
        os.environ['VLMEVALKIT_USE_MODELSCOPE'] = '1'
        def run_eval():
            task_cfg = "eval_tmp.yaml" #å¡«å…¥å¯¹åº”yamlé…ç½®æ–‡ä»¶è·¯å¾„
            run_task(task_cfg=task_cfg)
            print('>> Start to get the report with summarizer ...')
            report_list = Summarizer.get_report_from_cfg(task_cfg)
        run_eval()
      ```
<br>
<br>



## æ¨¡å‹ç¼–è¯‘

æœ¬èŠ‚ä»‹ç»é‡åŒ–å¤§æ¨¡å‹çš„ç¼–è¯‘ï¼Œç›®å‰åˆ†ä¸ºè¯­è¨€å¤§æ¨¡å‹å’Œè§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼Œç¼–è¯‘æ–¹å¼ç¨æœ‰ä¸åŒï¼Œä»¥ä¸‹é€šè¿‡è¯¦ç»†ç¤ºä¾‹ä»£ç è¯´æ˜ï¼š

### è¯­è¨€å¤§æ¨¡å‹

ä»¥``Qwen3-1.7B-AWQ``ä¸ºä¾‹ï¼š

```python
from tyllm.build_util import build_and_compile_llm

quant_path = "./Qwen3-1.7B-AWQ"
aot_path = f"{quant_path}-AOT"

# é¢„å¡«å……åºåˆ—é•¿åº¦
prefill_seq_len = 8
# æœ€å¤§KVé”®å€¼å¯¹æ•°ï¼Œæ§åˆ¶æ¨¡å‹æ¨ç†æœŸé—´ä¸Šä¸‹æ–‡é•¿åº¦
max_kv_cache_size = 4096
# æŒ‡å®šå¤šdieç¼–è¯‘ï¼Œå¤šdieå¹¶è¡Œè®¡ç®—
die_num = 4
# æ˜¯å¦å°†embeddingæ“ä½œä½œä¸ºè¾“å…¥ï¼Œé»˜è®¤Falseï¼›å¦‚æœTrueï¼Œembeddingè®¡ç®—å°†è¢«offloadåˆ°cpu
embedding_as_input = False

torch_edgex.set_device_trace_only("edgex", True)
torch_edgex.set_device_mode("LM_die_remap", [0,1,2,3,0,0,0,0,0,0,0,0,0,0,0,0])

build_and_compile_llm(
    model_path=quant_path,
    artifacts_path=f"{aot_path}_{prefill_seq_len}_{max_kv_cache_size}/{die_num}die",
    max_kv_cache_size=max_kv_cache_size,
    seq_len_list=[1, prefill_seq_len],
    dev_count=die_num,
    embedding_as_input=embedding_as_input,
)
```

**å‚æ•°è¯´æ˜**ï¼š
- **model_path(str)** ``huggingface``æ¨¡å‹å’Œé…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼›
- **max_kv_cache_size(int, optional)** ``kv``ç¼“å­˜çš„æœ€å¤§å®¹é‡ï¼Œé»˜è®¤ä¸º``4096``ï¼›
- **seq_len_list(list of int, optional)** ç”¨äºæ„å»ºå’Œç¼–è¯‘æ¨¡å‹çš„åºåˆ—é•¿åº¦åˆ—è¡¨ï¼Œé»˜è®¤ä¸º``[1, 8]``ï¼›
- **dev_count(int, optional)** ç”¨äºè¿è¡Œå·²ç¼–è¯‘æ¨¡å‹çš„è®¾å¤‡æ•°é‡ï¼ˆå¦‚ NNP è®¾å¤‡ï¼‰ï¼Œé»˜è®¤ä¸º``1``ï¼›
- **artifacts_path(str, optional)** ä¿å­˜æ¨¡å‹ç¼–è¯‘äº§ç‰©ï¼ˆå¦‚æƒé‡ã€åµŒå…¥å±‚ç­‰ï¼‰çš„ç›®å½•è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨``model_path``ä½œä¸ºé»˜è®¤è·¯å¾„ï¼›
- **embedding_as_input(bool, optional)** å¦‚æœä¸º``True``ï¼Œå°†æå–åµŒå…¥å±‚å¹¶å•ç‹¬ä¿å­˜ä¸º``NumPy``æ•°ç»„ï¼Œé»˜è®¤ä¸º``False``ï¼›

**ç¼–è¯‘åäº§ç‰©ç›®å½•**ï¼š

```shell
Qwen3-1.7B-AWQ-AOT/
â””â”€â”€ 1die
    â”œâ”€â”€ batch_1
    â”‚Â Â  â”œâ”€â”€ common_die0.params
    â”‚Â Â  â”œâ”€â”€constant_die0_1.params
    â”‚Â Â  â”œâ”€â”€constant_die0_8.params
    â”‚Â Â  â”œâ”€â”€ seqlen_1
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.params
    â”‚Â Â  â”‚Â Â  â””â”€â”€ llm_decode_die0.so
    â”‚Â Â  â””â”€â”€ seqlen_8
    â”‚Â Â      â”œâ”€â”€ llm_die0.params
    â”‚Â Â      â”œâ”€â”€ llm_decode_die0.so
    |       â””â”€â”€ llm_prefill_die0.so 
    â”œâ”€â”€ buffer_config.json
    â”œâ”€â”€ config.json
    â””â”€â”€ empty.bin
```

### è§†è§‰è¯­è¨€å¤§æ¨¡å‹

åŸºäº``vLLM``çš„``Qwen2.5-VL-7B-Instruct``ç¤ºä¾‹ï¼š

```python
import os
import logging
import datetime
import numpy as np
import torch
from PIL import Image
from vllm import LLM
from vllm.config import ModelConfig, ParallelConfig
torch.distributed.constants.default_pg_timeout = datetime.timedelta(hours=5)
from tyllm import torch_edgex
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
if torch.cuda.is_available():
    torch_edgex.set_device_mode('jit_device', 'cuda')
else:
    torch_edgex.set_device_mode('jit_device', 'cpu')
from tyllm.vllm_ext.edgex_executor import EdgeXExecutor
from vllm.platforms import current_platform
import shutil
import glob
import argparse
from vllm.config import ModelConfig

def list_to_str_without_tail_zeros(lst):
    last_non_zero_idx = -1
    for i in range(len(lst)-1, -1, -1):
        if lst[i] != 0:
            last_non_zero_idx = i
            break
    if last_non_zero_idx == -1:
        return ""
    return "".join(str(num) for num in lst[:last_non_zero_idx+1])


# å…¨å±€åˆå§‹åŒ–é…ç½®
ModelConfig.verify_with_parallel_config = lambda a, b: True

args = None
IMAGE_ORG_PATH = "./960_540.jpg" 
# é¢„å¤„ç†åçš„å›¾ç‰‡è·¯å¾„
IMAGE_PATH = "./test.jpg"  

# è®¾å¤‡é…ç½®
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
if torch.cuda.is_available():
    torch_edgex.set_device_mode('jit_device', 'cuda')
else:    
    torch_edgex.set_device_mode('jit_device', 'cpu')

os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["COMPILE_THREAD"] = "1"
logging.getLogger("vllm").setLevel(logging.WARNING)

# è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶åˆå§‹åŒ–å…¨å±€é…ç½®
def parse_args():
    global args
    parser = argparse.ArgumentParser(description="vLLMå¤šæ¨¡æ€æ¨ç†")
    parser.add_argument("--model_dir", type=str, default="./quantized_models/qwen3vl-4b-AWQ", help="é‡åŒ–æ¨¡å‹è·¯å¾„")
    parser.add_argument("--num_die", type=int, default=4, help="è®¾å¤‡æ•°é‡")
    parser.add_argument("--input_height", type=int, default=540, help="è¾“å…¥å›¾åƒé«˜åº¦")
    parser.add_argument("--input_width", type=int, default=960, help="è¾“å…¥å›¾åƒå®½åº¦")
    parser.add_argument("--modality", type=str, default="image", choices=["image", "video"], help="è¾“å…¥æ¨¡æ€")
    parser.add_argument("--source_tokenizer", type=str, default="./tokenizer.json", help="åŸæ¨¡å‹tokenizer.jsonæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--prefill_lens", type=int, default=96, help="é¢„å¡«å……åºåˆ—é•¿åº¦")
    parser.add_argument("--max_model_len", type=int, default=8192, help="æ¨¡å‹å•æ¬¡æ¨ç†ä¸­èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--vm_die_remap", type=int, default=[0,1,2,3,0,0,0,0,0,0,0,0,0,0,0,0,0], help="vit die remap")
    parser.add_argument("--lm_die_remap", type=int, default=[0,1,2,3,0,0,0,0,0,0,0,0,0,0,0,0,0], help="lm die remap")
    args = parser.parse_args()

# å‚æ•°éœ€åœ¨torch_edgexé…ç½®å‰å®Œæˆ
parse_args()
input_size = (args.input_height, args.input_width, 3)
remap = list_to_str_without_tail_zeros(args.vm_die_remap)+list_to_str_without_tail_zeros(args.lm_die_remap)
aot_dir = f"./compiled_models/Qwen3-VL-4b-AWQ-AOT_{input_size[1]}x{input_size[0]}_{args.max_model_len}_{args.num_die}die_{args.modality}_{remap}_gpu"

# é…ç½®torch_edgex
torch_edgex.edgex_module.set_trace_only_mode(True)
torch_edgex.set_device_mode("exec_mode", "AOT")
torch_edgex.set_device_mode("prefill_lens", [1, args.prefill_lens])
torch_edgex.set_device_mode("AOT_DIR", aot_dir)
torch_edgex.set_device_mode('tmp_image_path', IMAGE_PATH)
torch_edgex.set_device_mode("VM_die_remap", [3,2,1,0,0,0,0,0,0,0,0,0,0,0,0,0])
torch_edgex.set_device_mode("LM_die_remap", [1,2,3,0,0,0,0,0,0,0,0,0,0,0,0,0])

# åŠ¨æ€ä¿®æ”¹ParallelConfig
torch._dynamo.reset()
ModelConfig.verify_with_parallel_config = lambda a, b: True
origin_post_init = ParallelConfig.__post_init__

def modified_post_init(self):
    origin_post_init(self)
    self.world_size = 1

ParallelConfig.__post_init__ = modified_post_init

def main():
    global args, aot_dir, IMAGE_PATH
    
    # å›¾åƒé¢„å¤„ç†
    Image.open(IMAGE_ORG_PATH).resize((args.input_width, args.input_height)).save(IMAGE_PATH)

    # åˆ›å»ºç›®å½•
    mrope_dir = os.path.join(aot_dir, str(args.num_die)+"die", "mrope")
    visual_dir = os.path.join(aot_dir, str(args.num_die)+"die", "visual")
    for dir_path in [aot_dir, mrope_dir, visual_dir]:
        os.makedirs(dir_path, exist_ok=True)
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    modality = args.modality
    if modality == "image":
        data = Image.open(IMAGE_PATH)
    elif modality == "video":
        data = np.array([Image.open(IMAGE_PATH) for _ in range(10)])
    
    question = "è¯·æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹"
    placeholder = "<|image_pad|>" if modality == "image" else "<|video_pad|>"
    prompt = (
        "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
        f"<|im_start|>user\n<|vision_start|>{placeholder}<|vision_end|>"
        f"{question}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )
    
    # åˆå§‹åŒ–æ¨¡å‹
    llm = LLM(
        model=args.model_dir,
        max_model_len=args.max_model_len,
        tensor_parallel_size=args.num_die,
        max_num_seqs=5,
        mm_processor_kwargs={
            "min_pixels": 256 * 28 * 28,
            "max_pixels": 1280 * 28 * 28,
        },
        disable_mm_preprocessor_cache=True,
        trust_remote_code=True,
        dtype="float16", 
        disable_async_output_proc=True,
        distributed_executor_backend=EdgeXExecutor,
        worker_cls="tyllm.vllm_ext.edgex_executor.EdgeXWorker",
        device="cpu"
    )
    
    # æ‰§è¡Œç¼–è¯‘
    print("æ‰§è¡Œé¦–æ¬¡æ¨ç†ä»¥è§¦å‘AOTç¼–è¯‘...")
    inputs = {
        "prompt": prompt,
        "multi_modal_data": {modality: data},
    }
    _ = llm.generate(inputs, use_tqdm=False)
    print("ç¼–è¯‘å®Œæˆï¼Œå¼€å§‹å¤„ç†ç”Ÿæˆçš„æ–‡ä»¶...")
    
    # æ–‡ä»¶å¤„ç†
    try:
        # å¤„ç†mropeç›®å½•
        print(f"å¤„ç†{mrope_dir}ä¸‹çš„æ–‡ä»¶...")
        mrope_so_files = glob.glob(os.path.join(mrope_dir, "*.so"))
        mrope_params_files = glob.glob(os.path.join(mrope_dir, "*.params"))
        print(f"mrope_so_files={mrope_so_files}")
        print(f"mrope_params_files={mrope_params_files}")
        if mrope_so_files:
            shutil.copy2(mrope_so_files[0], os.path.join(aot_dir, str(args.num_die)+"die", "compute_rope_param.so"))
            print(f"å·²å¤åˆ¶å¹¶é‡å‘½åSOæ–‡ä»¶: {mrope_so_files[0]} -> {aot_dir}/compute_rope_param.so")
        
        if mrope_params_files:
            shutil.copy2(mrope_params_files[0], os.path.join(aot_dir,  str(args.num_die)+"die", "compute_rope_param.params"))
            print(f"å·²å¤åˆ¶å¹¶é‡å‘½åparamsæ–‡ä»¶: {mrope_params_files[0]} -> {aot_dir}/compute_rope_param.params")
        
        # å¤„ç†visualç›®å½•
        print(f"å¤„ç†{visual_dir}ä¸‹çš„æ–‡ä»¶...")
        
        # å¤„ç†é…ç½®æ–‡ä»¶å’Œso/paramsæ–‡ä»¶
        aot_config_files = glob.glob(os.path.join(visual_dir, "*aot_config.json"))
        if aot_config_files:
            os.replace(aot_config_files[0], os.path.join(visual_dir, "aot_config.json"))
        
        buffer_config_files = glob.glob(os.path.join(visual_dir, "*buffer_config.json"))
        if buffer_config_files:
            os.replace(buffer_config_files[0], os.path.join(visual_dir, "buffer_config.json"))
        
        # å¤„ç†die0-3çš„soå’Œparamsæ–‡ä»¶
        die_so_map = {f"die{i}.so": f"vit_die{i}.so" for i in range(4)}
        die_params_map = {f"die{i}.params": f"constant_die{i}.params" for i in range(4)}
        
        for src, dst in die_so_map.items():
            files = glob.glob(os.path.join(visual_dir, f"*{src}"))
            if files:
                os.replace(files[0], os.path.join(visual_dir, dst))
        
        for src, dst in die_params_map.items():
            files = glob.glob(os.path.join(visual_dir, f"*{src}"))
            if files:
                os.replace(files[0], os.path.join(visual_dir, dst))
        
        # å¤åˆ¶tokenizer
        if args.source_tokenizer and os.path.exists(args.source_tokenizer):
            shutil.copy2(args.source_tokenizer, os.path.join(aot_dir, str(args.num_die)+"die", "tokenizer.json"))
        
        print("æ–‡ä»¶å¤„ç†å®Œæˆ!")
        
    except Exception as e:
        print(f"æ–‡ä»¶å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
```

**è¯´æ˜**ï¼š
- æ–‡ä»¶å¤„ç†çš„ä»£ç å°†éƒ¨åˆ†ç”Ÿæˆäº§ç‰©æ”¹åï¼Œä»¥åŒ¹é…äº‘å¤©å¤§æ¨¡å‹SDKçš„æ–‡ä»¶åè¦æ±‚
- AOT_DIR è¯·é…ç½®ä¸º Qwen3-VL-4B... è¿™æ ·çš„æ¨¡å¼ä¸­é—´ç”¨-éš”å¼€ï¼Œä»¥åŒ¹é…æ¿ä¸Šè¿è¡Œäº‘å¤©å¤§æ¨¡å‹SDKçš„ç›®å½•åè¦æ±‚
- VM_die_remap é…ç½®è§†è§‰éƒ¨åˆ†Dieåˆ†é…é¡ºåºï¼Œlisté•¿åº¦å¿…é¡»ä¸º16
- LM_die_remap é…ç½®è¯­è¨€éƒ¨åˆ†Dieåˆ†é…é¡ºåºï¼Œlisté•¿åº¦å¿…é¡»ä¸º16
- IMAGE_ORG_PATHå¯ä»¥é…ç½®æœ¬åœ°ä»»æ„å…¶å®ƒå›¾ç‰‡ï¼Œåªä¸ºç”Ÿæˆå›¾åƒè¾“å…¥
- æ­¤è„šæœ¬GPUç¯å¢ƒä¸‹æ•´ä¸ªç¼–è¯‘è¿‡ç¨‹æœ‰ä¸€å®šåŠ é€Ÿï¼›å¦‚æ²¡æœ‰GPUå¯ä»¥å°è¯•"COMPILE_THREAD"é…ç½®ä¸º"2"ä»¥å¤šçº¿ç¨‹åŠ é€Ÿã€‚

**å¤‡æ³¨**
- æ¨¡å‹ç¼–è¯‘æ—¶ï¼Œä¼šæ‰§è¡Œä¸€æ¬¡forwardçš„traceè¿‡ç¨‹ï¼Œå¦‚æœæŠ¥ç¼ºå°‘config.jsonã€preprocessor_config.jsonã€vocab.jsonç­‰æ–‡ä»¶çš„é”™è¯¯ï¼Œå¯èƒ½æ˜¯é‡åŒ–è¿‡ç¨‹æ–‡ä»¶æ²¡æœ‰æ‹·è´å…¨ï¼Œæ­¤æ—¶ä»æµ®ç‚¹æ¨¡å‹ç›®å½•æ‰‹åŠ¨æ‹·è´ç›¸åº”çš„æ–‡ä»¶åˆ°é‡åŒ–æ¨¡å‹ç›®å½•å³å¯è§£å†³ã€‚
- æ¨¡å‹ç¼–è¯‘æ—¶ï¼Œå¦‚æœå› ä¸ºä¸€äº›é”™è¯¯å¯¼è‡´éœ€è¦é‡å¯ä»»åŠ¡ï¼Œå»ºè®®åˆ é™¤å·²ç”Ÿæˆçš„æ–‡ä»¶ï¼Œé‡æ–°è¿è¡Œè„šæœ¬ã€‚

**ç¼–è¯‘åäº§ç‰©ç±»ä¼¼å¦‚ä¸‹ç»“æ„**ï¼š

```shell
Qwen3-VL-4b-AWQ-AOT_960x540_8192_4die_image_01230123$ tree
.
â””â”€â”€ 4die
    â”œâ”€â”€ batch_1
    â”‚Â Â  â”œâ”€â”€ common_die0.params
    â”‚Â Â  â”œâ”€â”€ common_die1.params
    â”‚Â Â  â”œâ”€â”€ common_die2.params
    â”‚Â Â  â”œâ”€â”€ common_die3.params
    â”‚Â Â  â”œâ”€â”€ constant_die0_1.params
    â”‚Â Â  â”œâ”€â”€ constant_die0_8.params
    â”‚Â Â  â”œâ”€â”€ constant_die0_96.params
    â”‚Â Â  â”œâ”€â”€ constant_die1_1.params
    â”‚Â Â  â”œâ”€â”€ constant_die1_8.params
    â”‚Â Â  â”œâ”€â”€ constant_die1_96.params
    â”‚Â Â  â”œâ”€â”€ constant_die2_1.params
    â”‚Â Â  â”œâ”€â”€ constant_die2_8.params
    â”‚Â Â  â”œâ”€â”€ constant_die2_96.params
    â”‚Â Â  â”œâ”€â”€ constant_die3_1.params
    â”‚Â Â  â”œâ”€â”€ constant_die3_8.params
    â”‚Â Â  â”œâ”€â”€ constant_die3_96.params
    â”‚Â Â  â”œâ”€â”€ seqlen_1
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die0.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die1.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die2.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die3.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die0.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die1.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die2.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die3.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die1.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die2.params
    â”‚Â Â  â”‚Â Â  â””â”€â”€ llm_die3.params
    â”‚Â Â  â”œâ”€â”€ seqlen_8
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die0.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die1.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die2.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_decode_die3.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_die0.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_die1.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_die2.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_die3.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_prefill_die0.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_prefill_die1.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_prefill_die2.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ckpt_llm_prefill_die3.json
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die0.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die1.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die2.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_decode_die3.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die1.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die1.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die2.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die2.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die3.params
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die3.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_prefill_die0.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_prefill_die1.so
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_prefill_die2.so
    â”‚Â Â  â”‚Â Â  â””â”€â”€ llm_prefill_die3.so
    â”‚Â Â  â””â”€â”€ seqlen_96
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_die0.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_die1.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_die2.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_die3.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_prefill_die0.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_prefill_die1.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_prefill_die2.json
    â”‚Â Â      â”œâ”€â”€ ckpt_llm_prefill_die3.json
    â”‚Â Â      â”œâ”€â”€ llm_die0.params
    â”‚Â Â      â”œâ”€â”€ llm_die0.so
    â”‚Â Â      â”œâ”€â”€ llm_die1.params
    â”‚Â Â      â”œâ”€â”€ llm_die1.so
    â”‚Â Â      â”œâ”€â”€ llm_die2.params
    â”‚Â Â      â”œâ”€â”€ llm_die2.so
    â”‚Â Â      â”œâ”€â”€ llm_die3.params
    â”‚Â Â      â”œâ”€â”€ llm_die3.so
    â”‚Â Â      â”œâ”€â”€ llm_prefill_die0.so
    â”‚Â Â      â”œâ”€â”€ llm_prefill_die1.so
    â”‚Â Â      â”œâ”€â”€ llm_prefill_die2.so
    â”‚Â Â      â””â”€â”€ llm_prefill_die3.so
    â”œâ”€â”€ buffer_config.json
    â”œâ”€â”€ compute_rope_param.params
    â”œâ”€â”€ compute_rope_param.so
    â”œâ”€â”€ config.json
    â”œâ”€â”€ embedding.params
    â”œâ”€â”€ empty.bin
    â”œâ”€â”€ mrope
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_0.onnx
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_1.onnx
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_2.onnx
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_3.onnx
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_aot_config.json
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_buffer_config_0.json
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_buffer_config_1.json
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_buffer_config_2.json
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_buffer_config_3.json
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_buffer_config.json
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die0.params
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die0.so
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die1.params
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die1.so
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die2.params
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die2.so
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die3.params
    â”‚Â Â  â”œâ”€â”€ 3_8192_[int32]_die3.so
    â”‚Â Â  â””â”€â”€ 3_8192_[int32]_graph.json
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ visual
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_0.onnx
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_1.onnx
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_2.onnx
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_3.onnx
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_buffer_config_0.json
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_buffer_config_1.json
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_buffer_config_2.json
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_buffer_config_3.json
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_graph.json
        â”œâ”€â”€ 2040_1536_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]2040_1024_[float16]_preset_kwargs.pt
        â”œâ”€â”€ aot_config.json
        â”œâ”€â”€ buffer_config.json
        â”œâ”€â”€ constant_die0.params
        â”œâ”€â”€ constant_die1.params
        â”œâ”€â”€ constant_die2.params
        â”œâ”€â”€ constant_die3.params
        â”œâ”€â”€ vit_die0.so
        â”œâ”€â”€ vit_die1.so
        â”œâ”€â”€ vit_die2.so
        â””â”€â”€ vit_die3.so
```




## å¸¸è§é—®é¢˜

è‹¥åœ¨ä½¿ç”¨äº§å“è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒæ­¤æ–‡æ¡£ã€‚

### 1. ç¼–è¯‘é˜¶æ®µå‡ºç°segmentation fault(core dump)

**é—®é¢˜æè¿°**ï¼š

ç¼–è¯‘è¿‡ç¨‹ä¸­å¦‚æœå‡ºç°æ®µé”™è¯¯ï¼Œå¯èƒ½æ˜¯pyarrowåŒ…ç‰ˆæœ¬é—®é¢˜

**è§£å†³æ–¹æ³•**ï¼š

å°†``pyarrow``åŒ…é™çº§è‡³``16.0.0``ï¼Œ
```shell
pip install pyarrow==16.0.0
```

### 2. ç¼–è¯‘é˜¶æ®µå‡ºç° RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

**é—®é¢˜æè¿°**ï¼š
GPUç¯å¢ƒé¡»é™åˆ¶ç¼–è¯‘çº¿ç¨‹ç¯å¢ƒå˜é‡

**è§£å†³æ–¹æ³•**ï¼š
å°† os.environ["COMPILE_THREAD"] é…ç½®ä¸º "1"å³å¯è§£å†³ã€‚

### 3. ç¼–è¯‘é˜¶æ®µå‡ºç° Condition: status == DCL_ERROR_REPEAT_INITIALIZE failed

**é—®é¢˜æè¿°**ï¼š
æ£€æµ‹åˆ°GPUè®¾å¤‡æ—¶ï¼Œç¼–è¯‘è„šæœ¬å¯¹å…¨å±€è®¾ç½®æœ‰é¡ºåºè¦æ±‚

**è§£å†³æ–¹æ³•**ï¼š
ä¿æŒæ–‡æ¡£ä¸­ç¼–è¯‘è„šæœ¬å„æ“ä½œå…ˆåé¡ºåºä¸åšæ”¹åŠ¨ã€‚
