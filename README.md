

## æ›´æ–°è¯´æ˜

æœ¬æ–‡è®°å½•äº†``Edge10``å¤§æ¨¡å‹å·¥å…·é“¾``TyLLM``çš„å˜æ›´æƒ…å†µã€‚

**20250414/v1.0.8**

- ğŸš€ä¼˜åŒ–``Qwen2.5-VL-7B`` ``ViT``éƒ¨åˆ†æ€§èƒ½

**20250320/v1.0.6**

- ğŸš€æ–°å¢æ”¯æŒ``Qwen2.5-VL-7B``æ¨¡å‹``3Die``ç¼–è¯‘

**20250305/v0.0.2**

- ğŸš€æ–°å¢æ”¯æŒ``Qwen2.5-VL-7B``æ¨¡å‹``4Die/1Die``ç¼–è¯‘


## æ•´ä½“ä»‹ç»

``TyLLM``æ˜¯äº‘å¤©åŠ±é£æ¨å‡ºçš„å¤§æ¨¡å‹å·¥å…·é“¾ï¼Œå¯å¸®åŠ©ç”¨æˆ·å°†å¤§æ¨¡å‹ç¼–è¯‘ä¸º``Edge10``ç³»åˆ—èŠ¯ç‰‡ä¸Šæ‰§è¡Œçš„æ¨¡å‹ã€‚é™¤``TyLLM``è¿˜åŒ…æ‹¬``AutoAWQ``å’Œ``TyTVM``ï¼Œ``AutoAWQ``ä¸ºäº‘å¤©åŸºäºç¤¾åŒºå¼€æºç‰ˆæœ¬è‡ªè¡Œç»´æŠ¤çš„é‡åŒ–æ¨¡å—ï¼Œç›®çš„æ˜¯æœ€å¤§åŒ–æ¨¡å‹é‡åŒ–ï¼Œåœ¨``Edge10``ç³»åˆ—èŠ¯ç‰‡ä¸Šå–å¾—æœ€ä½³æ€§èƒ½ï¼›``TyLLM``ä¸ºäº‘å¤©åŸºäº``TyTVM``å·¥å…·é“¾é’ˆå¯¹å¤§æ¨¡å‹å¢é‡å¼€å‘çš„å·¥å…·ï¼Œä¸»è¦åŸºäº``PyTorch``å’Œ``vLLM``å¯¹å¤§æ¨¡å‹åšä¸“å±å’Œå®šåˆ¶ä¼˜åŒ–ï¼›``TyTVM``ä¸ºäº‘å¤©æ¨¡å‹è½¬æ¢ã€é‡åŒ–ã€ä»¿çœŸã€ç¼–è¯‘å·¥å…·é“¾ï¼Œä¸»è¦è´Ÿè´£å°†æ¨¡å‹ç¼–è¯‘ä¸ºèŠ¯ç‰‡æ‰§è¡Œçš„æ¨¡å‹ã€‚

### æ•´ä½“æ¶æ„å¦‚å›¾ï¼š

<div style="text-align:center;">
  <img src="./assets/whiteboard_exported_image.png" alt="æ¶æ„å›¾" style="width:100%; height:auto;" />
</div>

### æ¨¡å‹åˆ—è¡¨

å·²ç»æ”¯æŒçš„æ¨¡å‹å¦‚ä¸‹ï¼ˆåŒ…æ‹¬ä¸é™äºï¼‰ï¼š

| Model     | Support     |
| :-------- | :---------: |
| Qwen/Qwen2.5-VL-7B                            | âœ…     |
| Qwen/Qwen2-VL-7B                              | âœ…     |
| Qwen/Qwen2-7B                                 | âœ…     |
| Qwen/Qwen1.5-1.8B                             | âœ…     |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B     | âœ…     |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-7B       | âœ…     |
| deepseek-ai/DeepSeek-R1-Distill-Qwen-32B      | âœ…     |
| Llama3-8B                                     | âœ…     |


## å¿«é€Ÿå¼€å§‹

æœ¬èŠ‚ä»‹ç»ä½¿ç”¨``TyLLM``å·¥å…·é“¾å‰çš„å¼€å‘ç¯å¢ƒå‡†å¤‡å·¥ä½œã€‚``TyLLM``ä½¿ç”¨``Docker``å®¹å™¨è¿›è¡Œå·¥å…·é“¾é›†æˆï¼Œç”¨æˆ·å¯é€šè¿‡``Docker``åŠ è½½``TyLLM``é•œåƒæ–‡ä»¶ï¼Œç„¶åè¿›è¡Œæ¨¡å‹é‡åŒ–ã€ç¼–è¯‘ã€è¯„ä¼°(æœªæ¥)ç­‰å·¥ä½œï¼Œå› æ­¤å¼€å‘ç¯å¢ƒå‡†å¤‡é˜¶æ®µéœ€è¦æ­£ç¡®å®‰è£…``Docker``ç¯å¢ƒï¼ŒåŒæ—¶ç›®å‰éœ€è¦é‡åŒ–é˜¶æ®µéœ€è¦``GPU``æ¥åŠ é€Ÿï¼Œä»¥åŠå¤šæ¨¡æ€æ¨¡å‹çš„ç¼–è¯‘ä¾èµ–``vLLM``æ¡†æ¶æ¥æ¨ç†ï¼Œå› æ­¤æš‚æ—¶éœ€è¦``GPU``ã€‚

### ç¯å¢ƒå‡†å¤‡

- **Nvidia GPU**
- **Nvidia Container Toolkit**
- **Docker>19.03**

#### å®‰è£…Nvidia GPU é©±åŠ¨

```shell
sudo apt install nvidia-driver-530 # é©±åŠ¨ç‰ˆæœ¬å°½é‡é€‰æ‹©æœ€é«˜
# å®‰è£…å®Œæˆåï¼Œæ‰§è¡Œnvidia-smiå‘½ä»¤æ˜¾ç¤ºå¦‚ä¸‹ï¼Œè¡¨ç¤ºå®‰è£…æˆåŠŸã€‚
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.179                Driver Version: 535.179      CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3090        On  | 00000000:84:00.0 Off |                  N/A |
| 30%   27C    P8              23W / 350W |      3MiB / 24576MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```

#### å®‰è£…Docker

```shell
sudo apt install docker.io
sudo docker -v
# Docker version 20.10.21, build 20.10.21-0ubuntu1~20.04.2
```

#### å®‰è£…Nvidia Container Toolkit

æ·»åŠ åŒ…ä»“åº“å’Œ``GPG key``:
```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
    && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
    && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
             sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
             sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```

æ›´æ–°æºï¼Œå®‰è£…nvidia-container-toolkit

```shell
sudo apt update
sudo apt install nvidia-container-toolkit
```

#### å®‰è£…TyLLMå·¥å…·é“¾

å·¥å…·é“¾è·å–é€”å¾„å¦‚ä¸‹ï¼Œè¯·åŠ¡å¿…å°†``${version}``æ›¿æ¢ä¸ºå®é™…å¯¹åº”çš„å·¥å…·é“¾ç‰ˆæœ¬å·ï¼Œæ¯”å¦‚``v1.0.8``ï¼š

```shell
sudo docker login 113.100.143.90:8091 -u custom -p DE@sz_intellif_2021
sudo docker pull 113.100.143.90:8091/edgex/tyllm:${version}
```

> **æ³¨æ„**
> 
> éœ€è¦å°†``113.100.143.90:8091``åŠ å…¥``/etc/docker/daemon.json``ä¸­çš„``insecure-registries``å­—æ®µä¸­ï¼Œå¦‚ä¸‹ï¼š
> 
> ```json
> {     
>      "insecure-registries": ["113.100.143.90:8091"]
> }
>  ```
> ä¿®æ”¹åï¼Œé‡å¯``docker``ç”Ÿæ•ˆï¼Œ``sudo systemctl restart docker``

#### å¯åŠ¨å·¥å…·é“¾é•œåƒ

ä»¥ä¸‹å‘½ä»¤åˆ›å»ºå®¹å™¨ï¼Œå…¶ä¸­``${your_data_dir}``è¡¨ç¤ºå®¿ä¸»æœºä¸­ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œ``${version}``éœ€æ”¹ä¸ºå®é™…ç‰ˆæœ¬``tag``ã€‚
```shell
sudo docker run --gpus all -v ${your_data_dir}:/data -it 113.100.143.90:8091/edgex/tyllm:${version} bash
```

### æ¨¡å‹é‡åŒ–

äº‘å¤©çš„å¤§æ¨¡å‹é‡åŒ–æ˜¯é€šè¿‡``AutoAWQ``å·¥å…·å®Œæˆï¼Œè¯¥å·¥å…·æ˜¯äº‘å¤©åŸºäºç¤¾åŒºç‰ˆæœ¬å¼€å‘ç»´æŠ¤ï¼Œå…¶ç”¨æ³•ä¸ç¤¾åŒºç‰ˆæœ¬åŸºæœ¬ä¸€è‡´ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = "Qwen/Qwen2.5-VL-7B-Instruct"
quant_path = "./Qwen2.5-VL-7B-Instruct-AWQ-INT4"

model = AutoAWQForCausalLM.from_pretrained(model_path, torch_dtype="float16", cache_dir="./")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, cache_dir="./")

quant_config = {"zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM", "lm_head_enable": True}
model.quantize(tokenizer, quant_config=quant_config)
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)
```

> **æ³¨æ„**
> - å…¶ä¸­ï¼Œ``zero_point``ã€``q_group_size``ã€``w_bit``ã€``version``ä¸€èˆ¬å›ºå®šä¸ºç¤ºä¾‹ä¸­çš„å‚æ•°ï¼Œä»…``lm_head_enable``å‚æ•°éœ€è¦åº”æƒ…å†µè°ƒæ•´ï¼Œå¤§å¤šæƒ…å†µä¸‹ä¸º``True``ï¼Œå°‘éƒ¨åˆ†æ¨¡å‹å¯èƒ½ä¼šå½±å“èŠ¯ç‰‡ä¸Šæ¨ç†æ€§èƒ½ï¼›
> - åŠ è½½åŸå§‹æ¨¡å‹çš„æ—¶å€™éœ€è¦æ˜ç¡®æŒ‡å®š``torch_dtype="float16"``ï¼Œå› ä¸ºé»˜è®¤æ˜¯``"bfloat16"``ï¼Œå¦‚æœå­˜åœ¨éƒ¨åˆ†æœªé‡åŒ–çš„``module``ï¼Œå…¶``weight``ä»ç„¶æ˜¯``bfloat16``ï¼Œäº‘å¤©èŠ¯ç‰‡ä¸æ”¯æŒï¼›
> - æ¨¡å‹ä¸‹è½½å¯èƒ½éœ€è¦ç§‘å­¦ä¸Šç½‘ï¼›
> - ä¹Ÿå¯ä½¿ç”¨ç¤¾åŒºç‰ˆæœ¬è¿›è¡Œé‡åŒ–ï¼Œä½†éœ€è¦ç§»é™¤``lm_head_enable``ï¼›


### æ¨¡å‹ç¼–è¯‘

æœ¬èŠ‚ä¸»è¦ä»‹ç»é‡åŒ–å¤§æ¨¡å‹çš„ç¼–è¯‘ï¼Œç›®å‰åˆ†ä¸ºè¯­è¨€å¤§æ¨¡å‹å’Œè§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼Œç¼–è¯‘æ–¹å¼ç¨æœ‰ä¸åŒï¼Œä»¥ä¸‹é€šè¿‡è¯¦ç»†ç¤ºä¾‹ä»£ç è¯´æ˜ï¼š

#### è¯­è¨€å¤§æ¨¡å‹

ä»¥``Qwen1.5-1.8B``ä¸ºä¾‹ï¼š

```python
from tyllm.build_util import build_and_compile_llm

quant_path = "./Qwen1.5-1.8B-AWQ-INT4"
aot_path = f"{quant_path}-AOT"

# é¢„å¡«å……åºåˆ—é•¿åº¦
prefill_seq_len = 8
# æœ€å¤§KVé”®å€¼å¯¹æ•°ï¼Œæ§åˆ¶æ¨¡å‹æ¨ç†æœŸé—´ä¸Šä¸‹æ–‡é•¿åº¦
max_kv_cache_size = 4096
# æŒ‡å®šå¤šdieç¼–è¯‘ï¼Œå¤šdieå¹¶è¡Œè®¡ç®—
die_num = 4
# æ˜¯å¦å°†embeddingæ“ä½œä½œä¸ºè¾“å…¥ï¼Œé»˜è®¤Falseï¼›å¦‚æœTrueï¼Œembeddingè®¡ç®—å°†è¢«offloadåˆ°cpu
embedding_as_input = False

build_and_compile_llm(
    model_path=quant_path,
    artifacts_path=f"{aot_path}/{die_num}die",
    max_kv_cache_size=max_kv_cache_size,
    seq_len_list=[1, prefill_seq_len],
    dev_count=die_num,
    embedding_as_input=embedding_as_input,
)
```

**å‚æ•°è¯´æ˜**ï¼š

- **model_path(str)** ``huggingface``æ¨¡å‹å’Œé…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼›
- **max_kv_cache_size(int, optional)** ``kv``ç¼“å­˜çš„æœ€å¤§å®¹é‡ï¼Œé»˜è®¤ä¸º``4096``ï¼›
- **seq_len_list(list of int, optional)** ç”¨äºæ„å»ºå’Œç¼–è¯‘æ¨¡å‹çš„åºåˆ—é•¿åº¦åˆ—è¡¨ï¼Œé»˜è®¤ä¸º``[1, 8]``ï¼›
- **dev_count(int, optional)** ç”¨äºè¿è¡Œå·²ç¼–è¯‘æ¨¡å‹çš„è®¾å¤‡æ•°é‡ï¼ˆå¦‚ NNP è®¾å¤‡ï¼‰ï¼Œé»˜è®¤ä¸º``1``ï¼›
- **artifacts_path(str, optional)** ä¿å­˜æ¨¡å‹ç¼–è¯‘äº§ç‰©ï¼ˆå¦‚æƒé‡ã€åµŒå…¥å±‚ç­‰ï¼‰çš„ç›®å½•è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†ä½¿ç”¨``model_path``ä½œä¸ºé»˜è®¤è·¯å¾„ï¼›
- **embedding_as_input(bool, optional)** å¦‚æœä¸º``True``ï¼Œå°†æå–åµŒå…¥å±‚å¹¶å•ç‹¬ä¿å­˜ä¸º``NumPy``æ•°ç»„ï¼Œé»˜è®¤ä¸º``False``ï¼›

**ç¼–è¯‘åäº§ç‰©ç›®å½•**ï¼š

```shell
Qwen1.5-1.8B-AWQ-INT4-AOT/
â””â”€â”€ 1die
    â”œâ”€â”€ batch_1
    â”‚Â Â  â”œâ”€â”€ common_die0.params
    â”‚Â Â  â”œâ”€â”€ seqlen_1
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm_die0.params
    â”‚Â Â  â”‚Â Â  â””â”€â”€ llm_die0.so
    â”‚Â Â  â””â”€â”€ seqlen_8
    â”‚Â Â      â”œâ”€â”€ llm_die0.params
    â”‚Â Â      â””â”€â”€ llm_die0.so
    â”œâ”€â”€ buffer_config.json
    â”œâ”€â”€ config.json
    â””â”€â”€ empty.bin
```

#### è§†è§‰è¯­è¨€å¤§æ¨¡å‹

åŸºäº``vLLM``çš„``Qwen2.5-VL-7B-Instruct``ç¤ºä¾‹ï¼š

```python
import os
import logging
import numpy as np
import torch
import tvm
from PIL import Image
from vllm import LLM, SamplingParams
from vllm.config import ModelConfig, ParallelConfig
from vllm.platforms import current_platform
from tyllm.vllm_ext.edgex_executor import EdgeXExecutor
from tyllm import torch_edgex

CUR_DEVICE = "cuda" if current_platform.is_cuda() else "cpu"

os.environ["TOKENIZERS_PARALLELISM"] = "false"
logging.getLogger("vllm").setLevel(logging.WARNING)

# ä¸ºå·²ç»è¿‡AutoAWQé‡åŒ–åçš„æ¨¡å‹è·¯å¾„
model_dir = "./Qwen2.5-VL-7B-Instruct-AWQ-INT4"

# æŒ‡å®šå¤šdieç¼–è¯‘ï¼Œå¤šdieå¹¶è¡Œè®¡ç®—
num_die = 4
# é¢„å¡«å……åºåˆ—é•¿åº¦
prefill_lens = 96
# æ¨¡å‹åœ¨å•æ¬¡æ¨ç†ä¸­èƒ½å¤Ÿå¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥ + è¾“å‡ºçš„æ€» token æ•°é‡ï¼‰ã€‚
max_model_len = 4096
# æ¨¡å‹è§†è§‰éƒ¨åˆ†è¾“å…¥size
input_size = (540, 960, 3)  #(H, W, 3), (1080, 1920, 3), (720, 1280, 3), (540, 960, 3), (448, 768, 3)  (360, 640, 3)

save_dir = f"./Qwen2.5-VL-7B-Instruct-AWQ-INT4-AOT_{input_size[1]}x{input_size[0]}_{max_model_len}"

torch_edgex.edgex_module.set_trace_only_mode(True)
torch_edgex.set_device_mode("exec_mode", "AOT")
torch_edgex.set_device_mode("prefill_lens", [1, prefill_lens])

# è®¾ç½®è¾“å‡ºç›®å½•
torch_edgex.set_device_mode("AOT_DIR", save_dir)

# è®¾ç½®å¤šdieåºåˆ—ï¼Œå…¶ä¸­é¦–ä½è¡¨ç¤ºdie0è¡¨ç¤ºä¸»die
torch_edgex.set_device_mode("VM_die_remap", [3, 2, 1, 0])
torch_edgex.set_device_mode("LM_die_remap", [1, 2, 3, 0])

torch._dynamo.reset()
ModelConfig.verify_with_parallel_config = lambda a, b: True
origin_post_init = ParallelConfig.__post_init__


def modified_post_init(self):
    origin_post_init(self)
    self.world_size = 1


ParallelConfig.__post_init__ = modified_post_init


def main():
    modality = "image"
    if modality == "image":
        data = Image.fromarray(np.random.randint(0, 256, input_size, dtype=np.uint8))
    elif modality == "video":
        num_frames = 10
        data = np.array([Image.fromarray(np.random.randint(0, 256, input_size, dtype=np.uint8)) for _ in range(num_frames)])

    question = "è¯·æè¿°å›¾ç‰‡ä¸­çš„å†…å®¹"

    llm = LLM(
        model=model_dir,
        max_model_len=max_model_len,
        tensor_parallel_size=num_die,  # die
        # ä»¥ä¸‹å‚æ•°ä¸ç¡¬ä»¶æ— å…³
        max_num_seqs=5,
        # Note - mm_processor_kwargs can also be passed to generate/chat calls
        mm_processor_kwargs={
            "min_pixels": 256 * 28 * 28,
            "max_pixels": 1280 * 28 * 28,
        },
        disable_mm_preprocessor_cache=True,
        dtype="float16",
        trust_remote_code=True,
        disable_async_output_proc=True,
        distributed_executor_backend=EdgeXExecutor,
        worker_cls="tyllm.vllm_ext.edgex_executor.EdgeXWorker",
        device=CUR_DEVICE
    )

    if modality == "image":
        placeholder = "<|image_pad|>"
    elif modality == "video":
        placeholder = "<|video_pad|>"

    prompt = (
        "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
        f"<|im_start|>user\n<|vision_start|>{placeholder}<|vision_end|>"
        f"{question}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    # Single inference
    inputs = {
        "prompt": prompt,
        "multi_modal_data": {modality: data},
    }

    _ = llm.generate(inputs, use_tqdm=False)


if __name__ == "__main__":
    main()
```

> **æ³¨æ„**ï¼š
> 
> - ç¼–è¯‘å®Œæˆåï¼Œ``mrope``ç›®å½•ä¸‹``so``æ–‡ä»¶éœ€è¦æ‰‹åŠ¨å¤åˆ¶åˆ°**ä¸Šçº§**ç›®å½•ï¼Œå¹¶é‡å‘½åä¸º``compute_rope_param.so``
> - ç¼–è¯‘å®Œæˆåï¼Œ``mrope``ç›®å½•ä¸‹``params``æ–‡ä»¶éœ€è¦æ‰‹åŠ¨å¤åˆ¶åˆ°**ä¸Šçº§**ç›®å½•ï¼Œå¹¶é‡å‘½åä¸º``compute_rope_param.params``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*aot_config.json``åç¼€æ–‡ä»¶é‡å‘½åä¸º``aot_config.json``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*buffer_config.json``åç¼€æ–‡ä»¶é‡å‘½åä¸º``buffer_config.json``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*die0.so``åç¼€æ–‡ä»¶é‡å‘½åä¸º``vit_die0.so``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*die1.so``åç¼€æ–‡ä»¶é‡å‘½åä¸º``vit_die1.so``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*die2.so``åç¼€æ–‡ä»¶é‡å‘½åä¸º``vit_die2.so``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*die0.params``åç¼€æ–‡ä»¶é‡å‘½åä¸º``constant_die0.params``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*die1.params``åç¼€æ–‡ä»¶é‡å‘½åä¸º``constant_die1.params``
> - ç¼–è¯‘å®Œæˆåï¼Œ``visual``ç›®å½•ä¸‹``*die2.params``åç¼€æ–‡ä»¶é‡å‘½åä¸º``constant_die2.params``
> - éœ€è¦æ‰‹åŠ¨å¤åˆ¶åŸæ¨¡å‹ä¸­çš„``tokenizer.json``æ–‡ä»¶åˆ°æ¨¡å‹ç›®å½•ä¸‹

**ç¼–è¯‘åäº§ç‰©ç›®å½•ç»“æ„å¦‚ä¸‹**ï¼š

```shell
Qwen2.5-VL-7B-Instruct-finetune-AWQ-INT4-AOT_960x540_8192/
â”œâ”€â”€ 4die
    â”œâ”€â”€ batch_1
    â”‚   â”œâ”€â”€ common_die0.params
    â”‚   â”œâ”€â”€ common_die1.params
    â”‚   â”œâ”€â”€ common_die2.params
    â”‚   â”œâ”€â”€ common_die3.params
    â”‚   â”œâ”€â”€ seqlen_1
    â”‚   â”‚   â”œâ”€â”€ llm_die0.params
    â”‚   â”‚   â”œâ”€â”€ llm_die0.so
    â”‚   â”‚   â”œâ”€â”€ llm_die1.params
    â”‚   â”‚   â”œâ”€â”€ llm_die1.so
    â”‚   â”‚   â”œâ”€â”€ llm_die2.params
    â”‚   â”‚   â”œâ”€â”€ llm_die2.so
    â”‚   â”‚   â”œâ”€â”€ llm_die3.params
    â”‚   â”‚   â””â”€â”€ llm_die3.so
    â”‚   â””â”€â”€ seqlen_96
    â”‚       â”œâ”€â”€ llm_die0.params
    â”‚       â”œâ”€â”€ llm_die0.so
    â”‚       â”œâ”€â”€ llm_die1.params
    â”‚       â”œâ”€â”€ llm_die1.so
    â”‚       â”œâ”€â”€ llm_die2.params
    â”‚       â”œâ”€â”€ llm_die2.so
    â”‚       â”œâ”€â”€ llm_die3.params
    â”‚       â””â”€â”€ llm_die3.so
    â”œâ”€â”€ buffer_config.json
    â”œâ”€â”€ config.json
    â”œâ”€â”€ embedding.params
    â”œâ”€â”€ empty.bin
    â”œâ”€â”€ mrope
    â”‚   â”œâ”€â”€ 3_8192_[int32].onnx
    â”‚   â”œâ”€â”€ 3_8192_[int32]_aot_config.json
    â”‚   â”œâ”€â”€ 3_8192_[int32]_buffer_config.json
    â”‚   â”œâ”€â”€ 3_8192_[int32]_die0.params
    â”‚   â”œâ”€â”€ 3_8192_[int32]_die0.so
    â”‚   â””â”€â”€ 3_8192_[int32]_graph.json
    â””â”€â”€ visual
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16].onnx
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_aot_config.json
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_buffer_config.json
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die0.params
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die0.so
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die1.params
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die1.so
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die2.params
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die2.so
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die3.params
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_die3.so
        â”œâ”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_graph.json
        â””â”€â”€ 2584_1176_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]2584_1_1280_[float16]_preset_kwargs.pt
```

## å¸¸è§é—®é¢˜

è‹¥åœ¨ä½¿ç”¨äº§å“è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒæ­¤æ–‡æ¡£ã€‚

### ç¼–è¯‘é˜¶æ®µå‡ºç°segmentation fault(core dump)

**é—®é¢˜æè¿°**ï¼š

ç¼–è¯‘è¿‡ç¨‹ä¸­å¦‚æœå‡ºç°æ®µé”™è¯¯ï¼Œå¯èƒ½æ˜¯pyarrowåŒ…ç‰ˆæœ¬é—®é¢˜

**è§£å†³æ–¹æ³•**ï¼š

å°†``pyarrow``åŒ…é™çº§è‡³``16.0.0``ï¼Œ
```shell
pip install pyarrow==16.0.0
```

### <é—®é¢˜äºŒ>

è‹¥æœ‰å¿…è¦ï¼Œåˆ™åœ¨å¼€å¤´å…·ä½“æè¿°è¯¥é—®é¢˜ï¼Œæä¾›æ›´å¤šç»†èŠ‚ä¿¡æ¯ã€‚

**é—®é¢˜æè¿°**ï¼š

**è§£å†³æ–¹æ³•**ï¼š


### <é—®é¢˜ N>

è‹¥æœ‰å¿…è¦ï¼Œåˆ™åœ¨å¼€å¤´å…·ä½“æè¿°è¯¥é—®é¢˜ï¼Œæä¾›æ›´å¤šç»†èŠ‚ä¿¡æ¯ã€‚

**é—®é¢˜æè¿°**ï¼š

**è§£å†³æ–¹æ³•**ï¼š

